{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-4e25ee396bc7>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 10e-5\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "d1 = tf.layers.dense(x, units=1024,  activation=tf.nn.relu, kernel_initializer=initializer, name=\"d1\")\n",
    "dropout1 = tf.layers.dropout(d1, rate=0.1, training=is_training, name=\"dropout1\")\n",
    "d2 = tf.layers.dense(dropout1, units=1024, activation=tf.nn.relu, kernel_initializer=initializer, name=\"d2\")\n",
    "ln1 = tf.contrib.layers.layer_norm(d2)\n",
    "d3 = tf.layers.dense(ln1, units=1024,  activation=tf.nn.relu, kernel_initializer=initializer, name=\"d3\")\n",
    "dropout2 = tf.layers.dropout(d3, rate=0.1, training=is_training, name=\"dropout2\")\n",
    "ln2 = tf.contrib.layers.layer_norm(dropout2)\n",
    "d4 = tf.layers.dense(ln2, units=1024,  activation=tf.nn.relu, kernel_initializer=initializer, name=\"d4\")\n",
    "ln3 = tf.contrib.layers.layer_norm(d4)\n",
    "d5 = tf.layers.dense(ln3, units=1024,  activation=tf.nn.relu, kernel_initializer=initializer, name=\"d5\")\n",
    "d6 = tf.layers.dense(d5, units=10, name=\"d6\")\n",
    "y  = tf.nn.softmax(d6, name=\"y\")\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./hw3_1.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "CHECK_POINT_FILE_NAME = \"./hw3_1.ckpt\"\n",
    "try:\n",
    "    saver.restore(sess, CHECK_POINT_FILE_NAME)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:0  Accuracy =  0.9653  Loss =  1.5027733\n",
      "Training Step:500  Accuracy =  0.9686  Loss =  1.4704438\n",
      "Training Step:1000  Accuracy =  0.9709  Loss =  1.4858124\n",
      "Training Step:1500  Accuracy =  0.9736  Loss =  1.4916209\n",
      "Training Step:2000  Accuracy =  0.9693  Loss =  1.4647149\n"
     ]
    }
   ],
   "source": [
    "TRAIN_STEPS = 2000\n",
    "\n",
    "for i in range(TRAIN_STEPS+1):\n",
    "    batch = mnist.train.next_batch(128)\n",
    "    _, loss_value = sess.run((train_step, loss), feed_dict={x: batch[0], y_: batch[1]})\n",
    "    if i % 500 == 0:\n",
    "        is_training = False\n",
    "        print('Training Step:' + str(i) + '  Accuracy =  ' + \n",
    "              str(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})) + \n",
    "              '  Loss =  ' + str(loss_value))\n",
    "        is_training = True\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = saver.save(sess, CHECK_POINT_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./hw3_1.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, CHECK_POINT_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9591"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Accuracy before performing copression (Baseline Performance)\n",
    "sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1024) (784, 784) (1024, 1024) (1024, 1024)\n",
      "(1024, 1024) (1024, 1024) (1024, 1024) (1024, 1024)\n",
      "(1024, 1024) (1024, 1024) (1024, 1024) (1024, 1024)\n",
      "(1024, 1024) (1024, 1024) (1024, 1024) (1024, 1024)\n",
      "(1024, 1024) (1024, 1024) (1024, 1024) (1024, 1024)\n",
      "(1024, 10) (10, 10) (1024, 1024) (1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Calculate W hat based on different D value\n",
    "D_range = [10, 20, 50, 100, 200, 500, -1]\n",
    "\n",
    "Whs_list = {}\n",
    "target_weights = [\"d1/kernel:0\", \"d2/kernel:0\", \"d3/kernel:0\", \"d4/kernel:0\", \"d5/kernel:0\", \"d6/kernel:0\"]\n",
    "target_bias = [\"d1/bias:0\", \"d2/bias:0\", \"d3/bias:0\", \"d4/bias:0\", \"d5/bias:0\", \"d6/bias:0\"]\n",
    "\n",
    "bias_list = {}\n",
    "\n",
    "for d in D_range:\n",
    "    Whs_list[d] = {}\n",
    "\n",
    "for var in tf.trainable_variables():\n",
    "#     print(var.name)\n",
    "    if var.name in target_weights:\n",
    "        s, u, v = tf.linalg.svd(var, full_matrices=False)        \n",
    "        s_diag = tf.linalg.diag(s)\n",
    "        s_val = sess.run(s_diag)\n",
    "        print(var.shape, s_val.shape, u_val.shape, v_val.shape)\n",
    "        for d in D_range: \n",
    "            s_h = np.copy(s_val.reshape(1, -1)).reshape(s_val.shape)\n",
    "            if d > 0:\n",
    "                # set index > d to zero\n",
    "                # don't bother to set zero to u and v for col or row > D\n",
    "                # since as long as we turncate diag matrix, the element of W hat \n",
    "                # which row/col > D will be zero.\n",
    "                s_h[:,d:] = 0\n",
    "                s_h[d:,:] = 0\n",
    "            wh = tf.matmul(u, tf.matmul(s_h, v, adjoint_b=True))\n",
    "            Whs_list[d][var.name] = sess.run(wh)\n",
    "    elif var.name in target_bias:\n",
    "        bias_list[var.name] = sess.run(var)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1024)\n",
      "(1024,)\n",
      "(1024, 1024)\n",
      "(1024,)\n",
      "(1024, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# Check the weight and bias shape\n",
    "print(Whs_list[-1][\"d1/kernel:0\"].shape)\n",
    "print(bias_list[\"d1/bias:0\"].shape)\n",
    "\n",
    "print(Whs_list[-1][\"d5/kernel:0\"].shape)\n",
    "print(bias_list[\"d5/bias:0\"].shape)\n",
    "\n",
    "print(Whs_list[-1][\"d6/kernel:0\"].shape)\n",
    "print(bias_list[\"d6/bias:0\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give D, calculate the accuracy based on \n",
    "def test_acc(d):\n",
    "    \n",
    "    whs = Whs_list[d]\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    d1 = tf.layers.dense(x, units=1024,  activation=tf.nn.relu, trainable=False, \\\n",
    "                         kernel_initializer=tf.constant_initializer(whs[\"d1/kernel:0\"]), \\\n",
    "                         bias_initializer=tf.constant_initializer(bias_list[\"d1/bias:0\"]))\n",
    "    d2 = tf.layers.dense(d1, units=1024, activation=tf.nn.relu, trainable=False, \\\n",
    "                         kernel_initializer=tf.constant_initializer(whs[\"d2/kernel:0\"]), \\\n",
    "                         bias_initializer=tf.constant_initializer(bias_list[\"d2/bias:0\"]))\n",
    "    ln1 = tf.contrib.layers.layer_norm(d2)\n",
    "    d3 = tf.layers.dense(ln1, units=1024,  activation=tf.nn.relu, trainable=False, \\\n",
    "                         kernel_initializer=tf.constant_initializer(whs[\"d3/kernel:0\"]), \\\n",
    "                         bias_initializer=tf.constant_initializer(bias_list[\"d3/bias:0\"]))\n",
    "    ln2 = tf.contrib.layers.layer_norm(d3)\n",
    "    d4 = tf.layers.dense(ln2, units=1024,  activation=tf.nn.relu, trainable=False, \\\n",
    "                         kernel_initializer=tf.constant_initializer(whs[\"d4/kernel:0\"]), \\\n",
    "                         bias_initializer=tf.constant_initializer(bias_list[\"d4/bias:0\"]))\n",
    "    ln3 = tf.contrib.layers.layer_norm(d4)\n",
    "    d5 = tf.layers.dense(ln3, units=1024,  activation=tf.nn.relu, trainable=False, \\\n",
    "                         kernel_initializer=tf.constant_initializer(whs[\"d5/kernel:0\"]), \\\n",
    "                         bias_initializer=tf.constant_initializer(bias_list[\"d5/bias:0\"]))\n",
    "    d6 = tf.layers.dense(d5, units=10, trainable=False, \\\n",
    "                         kernel_initializer=tf.constant_initializer(whs[\"d6/kernel:0\"]), \\\n",
    "                         bias_initializer=tf.constant_initializer(bias_list[\"d6/bias:0\"]))\n",
    "    y  = tf.nn.softmax(d6)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    sess=tf.Session()\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "    acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "    print(\"D : {}, accuracy : {}\".format(d, acc)) \n",
    "    return acc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D : 10, accuracy : 0.10779999941587448\n",
      "D : 20, accuracy : 0.17020000517368317\n",
      "D : 50, accuracy : 0.39879998564720154\n",
      "D : 100, accuracy : 0.6262999773025513\n",
      "D : 200, accuracy : 0.7976999878883362\n",
      "D : 500, accuracy : 0.9585999846458435\n",
      "D : -1, accuracy : 0.9707000255584717\n"
     ]
    }
   ],
   "source": [
    "acc_results = [ test_acc(d) for d in D_range ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw the chart to show the relation between D and accuracy\n",
    "x = D_range[:]\n",
    "x[-1] = 1024\n",
    "plt.plot(x, acc_results, 'ro')\n",
    "\n",
    "plt.xlabel('D')\n",
    "plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 20) (1024, 20)\n",
      "(784, 1024)\n",
      "(1024, 20) (1024, 20)\n",
      "(1024, 1024)\n",
      "(1024, 20) (1024, 20)\n",
      "(1024, 1024)\n",
      "(1024, 20) (1024, 20)\n",
      "(1024, 1024)\n",
      "(1024, 20) (1024, 20)\n",
      "(1024, 1024)\n",
      "(1024, 10) (10, 10)\n",
      "(1024, 10)\n"
     ]
    }
   ],
   "source": [
    "# Now we build network with using fixed D = 20\n",
    "\n",
    "FIX_D = 20\n",
    "\n",
    "de_u = {}\n",
    "de_v = {}\n",
    "\n",
    "for var in tf.trainable_variables():\n",
    "    if var.name in target_weights:\n",
    "        s, u, v = tf.linalg.svd(var, full_matrices=False)        \n",
    "        s_diag = tf.linalg.diag(s)\n",
    "        v2 = tf.matmul(v, s_diag, adjoint_b=True)  \n",
    "        u_val = sess.run(u)\n",
    "        v2_val = sess.run(v)\n",
    "        u_val = u_val[:, :FIX_D]\n",
    "        v2_val = v2_val[:, :FIX_D]\n",
    "        print(u_val.shape, v2_val.shape)\n",
    "        print(tf.matmul(u_val, v2_val, adjoint_b=True).shape)\n",
    "        de_u[var.name] = u_val\n",
    "        de_v[var.name] = v2_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the network based on U V2\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "def fine_tune_model():\n",
    "        \n",
    "    with tf.variable_scope(\"fine_tune_model\", reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "        x = tf.placeholder(tf.float32, [None, 784])\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        U1 = tf.get_variable(\"ft_U1\", [784, FIX_D], initializer = tf.constant_initializer(de_u[\"d1/kernel:0\"]))\n",
    "        V1 = tf.get_variable(\"ft_V1\", [1024, FIX_D], initializer = tf.constant_initializer(de_v[\"d1/kernel:0\"]))    \n",
    "        b1 = tf.Variable(bias_list[\"d1/bias:0\"])\n",
    "\n",
    "        U2 = tf.get_variable(\"ft_U2\", [1024, FIX_D], initializer = tf.constant_initializer(de_u[\"d2/kernel:0\"]))\n",
    "        V2 = tf.get_variable(\"ft_V2\", [1024, FIX_D], initializer = tf.constant_initializer(de_v[\"d2/kernel:0\"]))    \n",
    "        b2 = tf.Variable(bias_list[\"d2/bias:0\"])\n",
    "        \n",
    "        U3 = tf.get_variable(\"ft_U3\", [1024, FIX_D], initializer = tf.constant_initializer(de_u[\"d3/kernel:0\"]))\n",
    "        V3 = tf.get_variable(\"ft_V3\", [1024, FIX_D], initializer = tf.constant_initializer(de_v[\"d3/kernel:0\"]))    \n",
    "        b3 = tf.Variable(bias_list[\"d3/bias:0\"])\n",
    "        \n",
    "        U4 = tf.get_variable(\"ft_U4\", [1024, FIX_D], initializer = tf.constant_initializer(de_u[\"d4/kernel:0\"]))\n",
    "        V4 = tf.get_variable(\"ft_V4\", [1024, FIX_D], initializer = tf.constant_initializer(de_v[\"d4/kernel:0\"]))    \n",
    "        b4 = tf.Variable(bias_list[\"d4/bias:0\"])\n",
    "        \n",
    "        U5 = tf.get_variable(\"ft_U5\", [1024, FIX_D], initializer = tf.constant_initializer(de_u[\"d5/kernel:0\"]))\n",
    "        V5 = tf.get_variable(\"ft_V5\", [1024, FIX_D], initializer = tf.constant_initializer(de_v[\"d5/kernel:0\"]))    \n",
    "        b5 = tf.Variable(bias_list[\"d5/bias:0\"])\n",
    "            \n",
    "        U6 = tf.get_variable(\"ft_U6\", [1024, 10], initializer = tf.constant_initializer(de_u[\"d6/kernel:0\"]))\n",
    "        V6 = tf.get_variable(\"ft_V6\", [10, 10], initializer = tf.constant_initializer(de_v[\"d6/kernel:0\"]))    \n",
    "        b6 = tf.Variable(bias_list[\"d6/bias:0\"])\n",
    "\n",
    "        d1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(x, tf.matmul(U1, V1, adjoint_b=True)), b1))\n",
    "        d2 = tf.nn.relu(tf.nn.bias_add(tf.matmul(d1, tf.matmul(U2, V2, adjoint_b=True)), b2))\n",
    "        ln1 = tf.contrib.layers.layer_norm(d2)    \n",
    "        d3 = tf.nn.relu(tf.nn.bias_add(tf.matmul(ln1, tf.matmul(U3, V3, adjoint_b=True)), b3))\n",
    "        ln2 = tf.contrib.layers.layer_norm(d3)\n",
    "        d4 = tf.nn.relu(tf.nn.bias_add(tf.matmul(ln2, tf.matmul(U4, V4, adjoint_b=True)), b4))\n",
    "        ln3 = tf.contrib.layers.layer_norm(d4)\n",
    "        d5 = tf.nn.relu(tf.nn.bias_add(tf.matmul(ln3, tf.matmul(U5, V5, adjoint_b=True)), b5))\n",
    "        d6 = tf.nn.relu(tf.nn.bias_add(tf.matmul(d5, tf.matmul(U6, V6, adjoint_b=True)), b6))\n",
    "        y  = tf.nn.softmax(d6)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "        \n",
    "        train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "        sess=tf.Session()\n",
    "        tf.global_variables_initializer().run(session=sess)\n",
    "        \n",
    "        return (sess, accuracy, x, y_, train_step, loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.12460000067949295\n"
     ]
    }
   ],
   "source": [
    "(sess, accuracy, x, y_, train_step, loss) = fine_tune_model()\n",
    "\n",
    "acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "print(\"accuracy : {}\".format(acc)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:0  Accuracy =  0.9508  Loss =  1.5148978\n",
      "Training Step:500  Accuracy =  0.9549  Loss =  1.532229\n",
      "Training Step:1000  Accuracy =  0.9537  Loss =  1.5188977\n",
      "Training Step:1500  Accuracy =  0.9551  Loss =  1.5048193\n",
      "Training Step:2000  Accuracy =  0.9591  Loss =  1.5033535\n"
     ]
    }
   ],
   "source": [
    "# fine-tune the new network\n",
    "TRAIN_STEPS = 4000\n",
    "\n",
    "for i in range(TRAIN_STEPS+1):\n",
    "    batch = mnist.train.next_batch(128)\n",
    "    _, loss_value = sess.run((train_step, loss), feed_dict={x: batch[0], y_: batch[1]})\n",
    "    if i % 500 == 0:\n",
    "        is_training = False\n",
    "        print('Training Step:' + str(i) + '  Accuracy =  ' + \n",
    "              str(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})) + \n",
    "              '  Loss =  ' + str(loss_value))\n",
    "        is_training = True\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9591000080108643\n"
     ]
    }
   ],
   "source": [
    "acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "print(\"accuracy : {}\".format(acc)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
