{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import librosa\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SNR \n",
    "def calculateSNR(st, st_h):\n",
    "    st_sum = np.sum(np.abs(st))\n",
    "    diff_sum = np.sum(np.abs(st-st_h))\n",
    "    return 10*math.log(st_sum**2/diff_sum**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "def read_data(path, prefix, files_to_load):\n",
    "    signal = np.zeros((513, 0))\n",
    "    noise = np.zeros((513, 0))\n",
    "    mixture = np.zeros((513, 0))\n",
    "    for i in range(files_to_load):\n",
    "        s, _=librosa.load('{}/{}s{:04d}.wav'.format(path, prefix, i), sr=None)\n",
    "        S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "        signal = np.hstack((signal, S))\n",
    "        s, _=librosa.load('{}/{}n{:04d}.wav'.format(path, prefix, i), sr=None)\n",
    "        S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "        noise = np.hstack((noise, S))\n",
    "        s, _=librosa.load('{}/{}x{:04d}.wav'.format(path, prefix, i), sr=None)\n",
    "        S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "        mixture = np.hstack((mixture , S))\n",
    "\n",
    "    return signal, noise, mixture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_LEN = 600\n",
    "# TRAIN_DATA_LEN = 1200\n",
    "signal, noise, mixture = read_data('timit-homework/tr', 'tr', TRAIN_DATA_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_signal, v_noise, v_mixture = read_data('timit-homework/v', 'v', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 58900)\n",
      "(513, 18610)\n"
     ]
    }
   ],
   "source": [
    "print(signal.shape)\n",
    "print(v_signal.shape)\n",
    "# Try to output~\n",
    "sr = 16000\n",
    "# librosa.output.write_wav(\"./test_concat.wav\", librosa.istft(signal, hop_length=512), sr)\n",
    "librosa.output.write_wav(\"./v_test_concat.wav\", librosa.istft(v_mixture, hop_length=512), sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "signal_abs = np.abs(signal)\n",
    "noise_abs = np.abs(noise)\n",
    "mixture_abs = np.abs(mixture)\n",
    "\n",
    "signal_T = signal.T\n",
    "noise_T = noise.T\n",
    "mixture_T = mixture.T\n",
    "signal_abs_T = signal_abs.T\n",
    "noise_abs_T = noise_abs.T\n",
    "mixture_abs_T = mixture_abs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation data \n",
    "# Preprocess Data\n",
    "v_signal_abs = np.abs(v_signal)\n",
    "v_noise_abs = np.abs(v_noise)\n",
    "v_mixture_abs = np.abs(v_mixture)\n",
    "\n",
    "v_signal_T = v_signal.T\n",
    "v_noise_T = v_noise.T\n",
    "v_mixture_T = v_mixture.T\n",
    "v_signal_abs_T = v_signal_abs.T\n",
    "v_noise_abs_T = v_noise_abs.T\n",
    "v_mixture_abs_T = v_mixture_abs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IBM \n",
    "def get_M(signal, noise):\n",
    "    M = np.zeros_like(signal)\n",
    "    (ht, wd) = M.shape\n",
    "    for i in range(ht):\n",
    "        for j in range(wd):\n",
    "            if signal_abs_T[i][j] > noise_abs_T[i][j]:\n",
    "                M[i][j] = 1\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_M = get_M(signal_abs_T, signal_abs_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_M = get_M(v_signal_abs_T, v_signal_abs_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58900, 513) (58900, 513)\n",
      "(18610, 513) (18610, 513)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check the shape of X(mixture) and M\n",
    "print(mixture_abs_T.shape, train_M.shape)\n",
    "print(v_mixture_abs_T.shape, v_M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing M \n",
    "signal_recover = train_M * mixture_T\n",
    "v_signal_recover = v_M * v_mixture_T\n",
    "sr = 16000\n",
    "# librosa.output.write_wav(\"./v_before_recover.wav\", librosa.istft(mixture, hop_length=512), sr)\n",
    "# librosa.output.write_wav(\"./v_recover.wav\", librosa.istft(v_signal_recover.T, hop_length=512), sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.732813943612374\n",
      "12.992788440628296\n"
     ]
    }
   ],
   "source": [
    "# Test SNR based on v_signal_recover\n",
    "print(calculateSNR(signal, signal_recover.T))\n",
    "print(calculateSNR(v_signal, v_signal_recover.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Hyper Parameter\n",
    "time_steps = 8   #\n",
    "num_input = 513   # total data input (513 channel)\n",
    "num_classes = 513 # total classes (513 channel)\n",
    "num_hidden = 128  # hidden layer num of features\n",
    "keep_prob = 0.8\n",
    "\n",
    "\n",
    "is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create batch data for training\n",
    "train_data = tf.data.Dataset.from_tensor_slices(tf.constant(mixture_abs_T))\n",
    "label_data = tf.data.Dataset.from_tensor_slices(tf.constant(M.reshape(-1, num_classes)))\n",
    "batch_data = tf.data.Dataset.zip((train_data, label_data)).repeat().batch(batch_size)\n",
    "\n",
    "iterator = batch_data.make_one_shot_iterator()\n",
    "next_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.variable_scope(\"rnn_model\", reuse=tf.AUTO_REUSE):\n",
    "    X = tf.placeholder(\"float\", [None, time_steps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    weight = tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "    bias = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "\n",
    "# batch_size*128*513\n",
    "# weights = [tf.Variable(tf.random_normal([num_hidden, num_classes])) for i in range(time_steps)]\n",
    "# biases = [tf.Variable(tf.random_normal([num_classes])) for i in range(time_steps)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    cell1 = tf.contrib.rnn.DropoutWrapper(tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(num_hidden), output_keep_prob=keep_prob)\n",
    "    cell2 = tf.contrib.rnn.DropoutWrapper(tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(num_hidden), output_keep_prob=keep_prob)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])    \n",
    "    # Try tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell\n",
    "    \n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)    \n",
    "#     print(outputs.shape, tf.reshape(outputs, (-1, time_steps*num_hidden)).shape, weight.shape)\n",
    "    ret = tf.matmul(tf.reshape(outputs, (-1, num_hidden)), weight) + bias\n",
    "    print(ret.shape)\n",
    "    return ret\n",
    "\n",
    "#     stack_outputs = tf.stack(outputs)    \n",
    "#     print(outputs.shape)\n",
    "\n",
    "#     ret_outputs = []\n",
    "#     for i in range(len(outputs)):        \n",
    "#         ret_outputs.append(tf.matmul(outputs[i], weights[i]) + biases[i])\n",
    "#     stack_outputs = tf.stack(ret_outputs)\n",
    "#     return tf.reshape(stack_outputs, (-1, num_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 513)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss, train_op\n",
    "with tf.variable_scope(\"rnn_model\", reuse=tf.AUTO_REUSE):\n",
    "    Y_pred = RNN(X, weights, biases)\n",
    "    loss = tf.losses.mean_squared_error(labels=Y, predictions=Y_pred)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Y_pred.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Padding, to \n",
    "def fit_RNN_input_dim(data):\n",
    "    (row, _) = data.shape\n",
    "    count = 0\n",
    "    while row % time_steps != 0:\n",
    "        data = np.vstack((data, np.zeros(num_input)))\n",
    "        row += 1\n",
    "        count += 1\n",
    "    \n",
    "    return data, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18610, 513)\n"
     ]
    }
   ],
   "source": [
    "print(v_mixture_abs_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18616, 513) (18616, 513)\n"
     ]
    }
   ],
   "source": [
    "(v_mixture_abs_T_fit, _) = fit_RNN_input_dim(v_mixture_abs_T)\n",
    "(v_M_fit, _) = fit_RNN_input_dim(v_M)\n",
    "print(v_mixture_abs_T_fit.shape, v_M_fit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial session\n",
    "sess=tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint\n",
    "saver = tf.train.Saver()\n",
    "CHECK_POINT_FILE_NAME = \"./hw3_2.ckpt\"\n",
    "\n",
    "# try:\n",
    "#     saver.restore(sess, CHECK_POINT_FILE_NAME)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:0  Validation Loss =  0.2508672  Train Loss =  0.10139629\n",
      "Training Step:1000  Validation Loss =  0.24971285  Train Loss =  0.06732198\n",
      "Training Step:2000  Validation Loss =  0.25109106  Train Loss =  0.07073683\n",
      "Training Step:3000  Validation Loss =  0.24781749  Train Loss =  0.07903202\n",
      "Training Step:4000  Validation Loss =  0.24766901  Train Loss =  0.09114778\n",
      "Training Step:5000  Validation Loss =  0.2498993  Train Loss =  0.1290096\n",
      "Training Step:6000  Validation Loss =  0.24618882  Train Loss =  0.08853917\n",
      "Training Step:7000  Validation Loss =  0.2513773  Train Loss =  0.0706407\n",
      "Training Step:8000  Validation Loss =  0.24566773  Train Loss =  0.05474038\n",
      "Training Step:9000  Validation Loss =  0.2476668  Train Loss =  0.07958381\n",
      "Training Step:10000  Validation Loss =  0.24634673  Train Loss =  0.074462295\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "training_steps = 10000\n",
    "for i in range(training_steps+1):\n",
    "    (batch_x, batch_y) = sess.run(next_batch)\n",
    "    batch_x = batch_x.reshape(-1, time_steps, num_input)\n",
    "#     print(batch_x.shape, batch_y.shape)\n",
    "    _, loss_value = sess.run((train_op, loss), feed_dict={X: batch_x, Y: batch_y})\n",
    "    if i % 1000 == 0:\n",
    "        is_training = False\n",
    "        print('Training Step:' + str(i) + '  Validation Loss =  ' + \n",
    "              str(sess.run(loss, feed_dict={X: v_mixture_abs_T_fit.reshape(-1, time_steps, num_input), Y: v_M_fit})) + \n",
    "              '  Train Loss =  ' + str(loss_value))\n",
    "        is_training = True        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1 GRUCell(128) / Timesteps 8  / dynamic_rnn\n",
    "Training Step:0  Validation Loss =  3.9349077  Train Loss =  4.484976\n",
    "Training Step:1000  Validation Loss =  0.2597179  Train Loss =  0.23647253\n",
    "Training Step:2000  Validation Loss =  0.2569354  Train Loss =  0.17934279\n",
    "Training Step:3000  Validation Loss =  0.25998798  Train Loss =  0.14066795\n",
    "Training Step:4000  Validation Loss =  0.26135945  Train Loss =  0.14986697\n",
    "Training Step:5000  Validation Loss =  0.2674995  Train Loss =  0.15539052\n",
    "Training Step:6000  Validation Loss =  0.2602887  Train Loss =  0.14658244\n",
    "Training Step:7000  Validation Loss =  0.28413093  Train Loss =  0.0959926\n",
    "Training Step:8000  Validation Loss =  0.26343125  Train Loss =  0.14114633\n",
    "Training Step:9000  Validation Loss =  0.2696621  Train Loss =  0.11511127\n",
    "Training Step:10000  Validation Loss =  0.26768386  Train Loss =  0.10903315\n",
    "\n",
    "\n",
    "Train SNR: 3.251641059081583\n",
    "Validation SNR: 5.827979878762203\n",
    "\n",
    "\n",
    "#### 2 CudnnCompatibleLSTMCell(128) / Timesteps 8  / dynamic_rnn\n",
    "Training Step:0  Validation Loss =  1.3731027  Train Loss =  1.2348267\n",
    "Training Step:1000  Validation Loss =  0.27806786  Train Loss =  0.26656747\n",
    "Training Step:2000  Validation Loss =  0.26507422  Train Loss =  0.16778608\n",
    "Training Step:3000  Validation Loss =  0.26399598  Train Loss =  0.12274204\n",
    "Training Step:4000  Validation Loss =  0.2550358  Train Loss =  0.16121742\n",
    "Training Step:5000  Validation Loss =  0.24873348  Train Loss =  0.14020343\n",
    "Training Step:6000  Validation Loss =  0.2562572  Train Loss =  0.1109365\n",
    "Training Step:7000  Validation Loss =  0.24956793  Train Loss =  0.09588985\n",
    "Training Step:8000  Validation Loss =  0.2560808  Train Loss =  0.09700974\n",
    "Training Step:9000  Validation Loss =  0.24594867  Train Loss =  0.08684269\n",
    "Training Step:10000  Validation Loss =  0.2511251  Train Loss =  0.10253517\n",
    "\n",
    "Train SNR: 5.780295033543666\n",
    "Validation SNR: 9.500670758379465\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./hw3_2.ckpt'"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, CHECK_POINT_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get M for traiing data\n",
    "def recover_train(input_mixture, mixture_abs_T):\n",
    "    (mixture_fit, diff) = fit_RNN_input_dim(mixture_abs_T)\n",
    "    input_x = mixture_fit.reshape(-1, time_steps, num_input)\n",
    "    train_M = sess.run(Y_pred, feed_dict={X: input_x})\n",
    "    print(train_M.shape)\n",
    "    return train_M[:-diff] * input_mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58904, 513)\n"
     ]
    }
   ],
   "source": [
    "recover_signal = recover_train(mixture_T, mixture_abs_T)\n",
    "# librosa.output.write_wav(\"./train_recover.wav\", librosa.istft(recover_signal.T, hop_length=512), sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18616, 513)\n"
     ]
    }
   ],
   "source": [
    "# Recover Validaion track!\n",
    "v_recover_signal = recover_train(v_mixture_T, v_mixture_abs_T)\n",
    "librosa.output.write_wav(\"./v_recover_rnn.wav\", librosa.istft(v_recover_signal.T, hop_length=512), sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.780295033543666\n",
      "9.500670758379465\n"
     ]
    }
   ],
   "source": [
    "# Calculate SNR for training data and validation data\n",
    "print(calculateSNR(signal.T, recover_signal))\n",
    "print(calculateSNR(v_signal.T, v_recover_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
